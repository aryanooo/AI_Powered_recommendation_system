{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcade24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from recommender.collaborative import CollaborativeRecommender\n",
    "from recommender.hybrid import HybridRecommender\n",
    "from recommender.data_loader import load_ratings\n",
    "from recommender.evaluation import rmse, mae, precision_at_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4d61d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ratings = load_ratings()\n",
    "\n",
    "train, test = train_test_split(\n",
    "    ratings,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train.head(), test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c28ad1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Quick hack: temporarily overwrite ratings.csv with train\n",
    "# or better: modify CollaborativeRecommender to accept a ratings df.\n",
    "# Here weâ€™ll manually inject.\n",
    "\n",
    "collab = CollaborativeRecommender(cf_mode=\"user\", use_nmf=True)\n",
    "collab.ratings = train\n",
    "collab.items = collab.items or None  # items will be loaded in fit()\n",
    "collab.user_item_matrix = None\n",
    "collab.similarity_matrix = None\n",
    "collab.user_factors = None\n",
    "collab.item_factors = None\n",
    "\n",
    "# We slightly patch: call internal logic but using train\n",
    "from recommender.data_loader import create_user_item_matrix\n",
    "collab.items = collab.items or pd.read_csv(Path(\"..\") / \"data\" / \"items.csv\")\n",
    "collab.user_item_matrix = create_user_item_matrix(train)\n",
    "\n",
    "matrix_filled = collab.user_item_matrix.fillna(0).values\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "collab.similarity_matrix = cosine_similarity(matrix_filled)  # user-based\n",
    "\n",
    "# NMF part\n",
    "from sklearn.decomposition import NMF\n",
    "nmf_input = collab.user_item_matrix.fillna(0).values\n",
    "collab.nmf_model = NMF(\n",
    "    n_components=20,\n",
    "    init=\"random\",\n",
    "    random_state=42,\n",
    "    max_iter=200\n",
    ")\n",
    "collab.user_factors = collab.nmf_model.fit_transform(nmf_input)\n",
    "collab.item_factors = collab.nmf_model.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de66147",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def predict_cf(row):\n",
    "    return collab.predict_rating_cf(row[\"user_id\"], row[\"item_id\"])\n",
    "\n",
    "def predict_nmf(row):\n",
    "    return collab.predict_rating_nmf(row[\"user_id\"], row[\"item_id\"])\n",
    "\n",
    "def predict_hybrid(row, alpha=0.6):\n",
    "    cf_pred = collab.predict_rating_cf(row[\"user_id\"], row[\"item_id\"])\n",
    "    nmf_pred = collab.predict_rating_nmf(row[\"user_id\"], row[\"item_id\"])\n",
    "    if cf_pred is None and nmf_pred is None:\n",
    "        return None\n",
    "    if cf_pred is None:\n",
    "        return nmf_pred\n",
    "    if nmf_pred is None:\n",
    "        return cf_pred\n",
    "    return alpha * cf_pred + (1 - alpha) * nmf_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5222acca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_sample = test.copy().reset_index(drop=True)\n",
    "\n",
    "test_sample[\"pred_cf\"] = test_sample.apply(predict_cf, axis=1)\n",
    "test_sample[\"pred_nmf\"] = test_sample.apply(predict_nmf, axis=1)\n",
    "test_sample[\"pred_hybrid\"] = test_sample.apply(predict_hybrid, axis=1)\n",
    "\n",
    "# Drop rows where all predictions are None\n",
    "mask_valid = test_sample[[\"pred_cf\", \"pred_nmf\", \"pred_hybrid\"]].notna().any(axis=1)\n",
    "test_sample = test_sample[mask_valid]\n",
    "\n",
    "test_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3721b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y_true = test_sample[\"rating\"].values\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "for name, col in [\n",
    "    (\"CF\", \"pred_cf\"),\n",
    "    (\"NMF\", \"pred_nmf\"),\n",
    "    (\"Hybrid\", \"pred_hybrid\")\n",
    "]:\n",
    "    preds = test_sample[col].values\n",
    "    metrics[name] = {\n",
    "        \"RMSE\": rmse(y_true, preds),\n",
    "        \"MAE\": mae(y_true, preds)\n",
    "    }\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceff4b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def precision_at_k_for_model(df, model_col, k=10, threshold=4.0):\n",
    "    precisions = []\n",
    "\n",
    "    for user_id, group in df.groupby(\"user_id\"):\n",
    "        # True relevant items (ground truth)\n",
    "        relevant = group[group[\"rating\"] >= threshold][\"item_id\"].tolist()\n",
    "\n",
    "        # Recommended items based on predicted scores\n",
    "        sorted_group = group.sort_values(by=model_col, ascending=False)\n",
    "        recommended = sorted_group[\"item_id\"].tolist()\n",
    "\n",
    "        if not relevant:\n",
    "            continue  # skip users with no relevant items in test\n",
    "\n",
    "        p_at_k = precision_at_k(recommended, relevant, k=k)\n",
    "        precisions.append(p_at_k)\n",
    "\n",
    "    if not precisions:\n",
    "        return 0.0\n",
    "    return float(np.mean(precisions))\n",
    "\n",
    "prec_cf = precision_at_k_for_model(test_sample, \"pred_cf\", k=5)\n",
    "prec_nmf = precision_at_k_for_model(test_sample, \"pred_nmf\", k=5)\n",
    "prec_hybrid = precision_at_k_for_model(test_sample, \"pred_hybrid\", k=5)\n",
    "\n",
    "precision_results = {\n",
    "    \"CF\": prec_cf,\n",
    "    \"NMF\": prec_nmf,\n",
    "    \"Hybrid\": prec_hybrid\n",
    "}\n",
    "precision_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a90fc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "summary = []\n",
    "\n",
    "for model_name in [\"CF\", \"NMF\", \"Hybrid\"]:\n",
    "    summary.append({\n",
    "        \"Model\": model_name,\n",
    "        \"RMSE\": metrics[model_name][\"RMSE\"],\n",
    "        \"MAE\": metrics[model_name][\"MAE\"],\n",
    "        \"Precision@5\": precision_results[model_name]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
